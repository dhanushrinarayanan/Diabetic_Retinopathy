{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MgB7r6iB5nG4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn import svm\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import joblib # Import joblib directly instead of from sklearn.externals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZtBucj0w5_TF",
        "outputId": "3fb25812-3316-4dd6-d86d-bfe9143ecc23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFSUefb76B-W",
        "outputId": "2ee9b51c-0a6e-45c2-a7cc-612d1bb25c9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Severe DR', 'Proliferate DR', 'Mild DR', 'Moderate DR', 'Healthy']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Path to the main directory containing your folders\n",
        "main_folder_path = '/content/drive/MyDrive/Dataset2'\n",
        "\n",
        "# List all folders in the main directory\n",
        "folders = [f for f in os.listdir(main_folder_path) if os.path.isdir(os.path.join(main_folder_path, f))]\n",
        "print(folders)  # This will print the names of your folders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iIPXNh2z6LLB",
        "outputId": "6e636d3a-9067-43d0-f69c-3302e2e84a6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Healthy images: ['Healthy_476.png', 'Healthy_473.png', 'Healthy_474.png', 'Healthy_472.png', 'Healthy_475.png', 'Healthy_469.png', 'Healthy_478.png', 'Healthy_479.png', 'Healthy_484.png', 'Healthy_490.png', 'Healthy_493.png', 'Healthy_471.png', 'Healthy_487.png', 'Healthy_495.png', 'Healthy_468.png', 'Healthy_486.png', 'Healthy_483.png', 'Healthy_520.png', 'Healthy_517.png', 'Healthy_481.png', 'Healthy_4.png', 'Healthy_3.png', 'Healthy_555.png', 'Healthy_22.png', 'Healthy_2.png', 'Healthy_21.png', 'Healthy_20.png', 'Healthy_18.png', 'Healthy_19.png', 'Healthy_17.png', 'Healthy_16.png', 'Healthy_17(1).png', 'Healthy_15.png', 'Healthy_775.png', 'Healthy_798.png', 'Healthy_783.png', 'Healthy_787.png', 'Healthy_773.png', 'Healthy_800.png', 'Healthy_777.png']\n",
            "Mild DR images: ['Mild_DR_250.png', 'Mild_DR_260.png', 'Mild_DR_263.png', 'Mild_DR_244.png', 'Mild_DR_257.png', 'Mild_DR_255.png', 'Mild_DR_266.png', 'Mild_DR_276.png', 'Mild_DR_261.png', 'Mild_DR_247.png', 'Mild_DR_251.png', 'Mild_DR_270.png', 'Mild_DR_254.png', 'Mild_DR_275.png', 'Mild_DR_273.png', 'Mild_DR_277.png', 'Mild_DR_258.png', 'Mild_DR_256.png', 'Mild_DR_274.png', 'Mild_DR_272.png', 'Mild_DR_227.png', 'Mild_DR_265.png', 'Mild_DR_268.png', 'Mild_DR_264.png', 'Mild_DR_228.png', 'Mild_DR_253.png', 'Mild_DR_252.png', 'Mild_DR_262.png', 'Mild_DR_267.png', 'Mild_DR_245.png', 'Mild_DR_248.png', 'Mild_DR_269.png', 'Mild_DR_246.png', 'Mild_DR_259.png', 'Mild_DR_249.png', 'Mild_DR_240.png', 'Mild_DR_229.png', 'Mild_DR_231.png', 'Mild_DR_235.png', 'Mild_DR_225.png', 'Mild_DR_223.png', 'Mild_DR_243.png', 'Mild_DR_242.png', 'Mild_DR_222.png', 'Mild_DR_221.png', 'Mild_DR_271.png', 'Mild_DR_232.png', 'Mild_DR_241.png', 'Mild_DR_234.png', 'Mild_DR_226.png', 'Mild_DR_233.png', 'Mild_DR_239.png', 'Mild_DR_236.png', 'Mild_DR_224.png', 'Mild_DR_230.png', 'Mild_DR_238.png', 'Mild_DR_237.png']\n",
            "Moderate DR images: ['Moderate_DR_381.png', 'Moderate_DR_385.png', 'Moderate_DR_375.png', 'Moderate_DR_361.png', 'Moderate_DR_350.png', 'Moderate_DR_383.png', 'Moderate_DR_365.png', 'Moderate_DR_363.png', 'Moderate_DR_373.png', 'Moderate_DR_358.png', 'Moderate_DR_390.png', 'Moderate_DR_527.png', 'Moderate_DR_528.png', 'Moderate_DR_522.png', 'Moderate_DR_524.png', 'Moderate_DR_519.png', 'Moderate_DR_523.png', 'Moderate_DR_521.png', 'Moderate_DR_525.png', 'Moderate_DR_515.png', 'Moderate_DR_490.png', 'Moderate_DR_517.png', 'Moderate_DR_526.png', 'Moderate_DR_483.png', 'Moderate_DR_486.png', 'Moderate_DR_516.png', 'Moderate_DR_489.png', 'Moderate_DR_499.png', 'Moderate_DR_518.png', 'Moderate_DR_520.png', 'Moderate_DR_529.png', 'Moderate_DR_496.png', 'Moderate_DR_501.png', 'Moderate_DR_507.png', 'Moderate_DR_495.png', 'Moderate_DR_503.png', 'Moderate_DR_497.png', 'Moderate_DR_500.png', 'Moderate_DR_511.png', 'Moderate_DR_514.png', 'Moderate_DR_512.png', 'Moderate_DR_492.png', 'Moderate_DR_494.png', 'Moderate_DR_484.png', 'Moderate_DR_510.png', 'Moderate_DR_498.png', 'Moderate_DR_477.png', 'Moderate_DR_459.png', 'Moderate_DR_470.png', 'Moderate_DR_488.png', 'Moderate_DR_462.png', 'Moderate_DR_506.png', 'Moderate_DR_456.png', 'Moderate_DR_493.png', 'Moderate_DR_509.png', 'Moderate_DR_505.png', 'Moderate_DR_458.png', 'Moderate_DR_508.png', 'Moderate_DR_460.png', 'Moderate_DR_491.png', 'Moderate_DR_502.png', 'Moderate_DR_471.png', 'Moderate_DR_467.png', 'Moderate_DR_478.png', 'Moderate_DR_504.png', 'Moderate_DR_487.png', 'Moderate_DR_513.png', 'Moderate_DR_474.png', 'Moderate_DR_454.png', 'Moderate_DR_485.png', 'Moderate_DR_455.png']\n",
            "Proliferate DR images: ['Proliferate DR_24.png', 'Proliferate DR_23.png', 'Proliferate DR_25.png', 'Proliferate DR_22.png', 'Proliferate DR_20.png', 'Proliferate DR_21.png', 'Proliferate DR_19.png', 'Proliferate DR_18.png', 'Proliferate DR_161.png', 'Proliferate DR_156.png', 'Proliferate DR_158.png', 'Proliferate DR_153.png', 'Proliferate DR_159.png', 'Proliferate DR_157.png', 'Proliferate DR_154.png', 'Proliferate DR_149.png', 'Proliferate DR_150.png', 'Proliferate DR_152.png', 'Proliferate DR_116.png', 'Proliferate DR_119.png', 'Proliferate DR_141.png', 'Proliferate DR_143.png', 'Proliferate DR_155.png', 'Proliferate DR_135.png', 'Proliferate DR_133.png', 'Proliferate DR_129.png', 'Proliferate DR_140.png', 'Proliferate DR_146.png', 'Proliferate DR_126.png', 'Proliferate DR_136.png', 'Proliferate DR_160.png', 'Proliferate DR_138.png', 'Proliferate DR_131.png', 'Proliferate DR_115.png', 'Proliferate DR_151.png', 'Proliferate DR_120.png', 'Proliferate DR_116(1).png', 'Proliferate DR_115(1).png', 'Proliferate DR_122.png', 'Proliferate DR_132.png', 'Proliferate DR_137.png', 'Proliferate DR_127.png', 'Proliferate DR_118.png', 'Proliferate DR_134.png', 'Proliferate DR_145.png', 'Proliferate DR_128.png', 'Proliferate DR_144.png', 'Proliferate DR_123.png', 'Proliferate DR_117.png', 'Proliferate DR_125.png', 'Proliferate DR_142.png', 'Proliferate DR_111(1).png', 'Proliferate DR_113.png', 'Proliferate DR_114.png', 'Proliferate DR_147.png', 'Proliferate DR_108(1).png', 'Proliferate DR_109(1).png', 'Proliferate DR_104.png', 'Proliferate DR_106.png', 'Proliferate DR_109.png', 'Proliferate DR_103.png', 'Proliferate DR_111.png', 'Proliferate DR_110(1).png', 'Proliferate DR_148.png', 'Proliferate DR_139.png', 'Proliferate DR_121.png', 'Proliferate DR_112.png', 'Proliferate DR_108.png', 'Proliferate DR_100.png', 'Proliferate DR_105.png', 'Proliferate DR_124.png', 'Proliferate DR_130.png', 'Proliferate DR_102.png', 'Proliferate DR_110.png', 'Proliferate DR_112(1).png', 'Proliferate DR_107.png', 'Proliferate DR_114(1).png', 'Proliferate DR_113(1).png', 'Proliferate DR_101.png']\n",
            "Severe DR images: ['Severe DR_48.png', 'Severe DR_26.png', 'Severe DR_5.png', 'Severe DR_54.png', 'Severe DR_27.png', 'Severe DR_24.png', 'Severe DR_33.png', 'Severe DR_30.png', 'Severe DR_56.png', 'Severe DR_64.png', 'Severe DR_47.png', 'Severe DR_34.png', 'Severe DR_42.png', 'Severe DR_43.png', 'Severe DR_46.png', 'Severe DR_25.png', 'Severe DR_23.png', 'Severe DR_40.png', 'Severe DR_92.png', 'Severe DR_21.png', 'Severe DR_41.png', 'Severe DR_39.png', 'Severe DR_2.png', 'Severe DR_53.png', 'Severe DR_22.png', 'Severe DR_28.png', 'Severe DR_36.png', 'Severe DR_3.png', 'Severe DR_31.png', 'Severe DR_44.png', 'Severe DR_32.png', 'Severe DR_20.png', 'Severe DR_13.png', 'Severe DR_19.png', 'Severe DR_17.png', 'Severe DR_37.png', 'Severe DR_18.png', 'Severe DR_16.png', 'Severe DR_14.png', 'Severe DR_15.png', 'Severe DR_100.png', 'Severe DR_103.png', 'Severe DR_10.png', 'Severe DR_12.png', 'Severe DR_11.png', 'Severe DR_101.png', 'Severe DR_102.png', 'Severe DR_114.png']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Path to your main directory in Google Drive\n",
        "path_data = '/content/drive/MyDrive/Dataset2'  # Update this path\n",
        "\n",
        "# List the main folder contents\n",
        "data = os.listdir(path_data)\n",
        "\n",
        "# List each category folder's contents\n",
        "healthy = os.listdir(os.path.join(path_data, 'Healthy'))\n",
        "mild = os.listdir(os.path.join(path_data, 'Mild DR'))\n",
        "moderate = os.listdir(os.path.join(path_data, 'Moderate DR'))\n",
        "proliferate = os.listdir(os.path.join(path_data, 'Proliferate DR'))\n",
        "severe = os.listdir(os.path.join(path_data, 'Severe DR'))\n",
        "\n",
        "# Optional: Print the contents for verification\n",
        "print(\"Healthy images:\", healthy)\n",
        "print(\"Mild DR images:\", mild)\n",
        "print(\"Moderate DR images:\", moderate)\n",
        "print(\"Proliferate DR images:\", proliferate)\n",
        "print(\"Severe DR images:\", severe)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCbe5n2J6Nhw",
        "outputId": "d6a34888-b42d-4a03-a3e9-09760769035b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classes names : ['Severe DR', 'Proliferate DR', 'Mild DR', 'Moderate DR', 'Healthy'] \n",
            "______________________________\n",
            "\n",
            "Number of classes : 5 \n",
            "______________________________\n",
            "\n",
            "Number of Healty images : 40 \n",
            "______________________________\n",
            "\n",
            "Number of Mild images : 57 \n",
            "______________________________\n",
            "\n",
            "Number of Moderate images : 71 \n",
            "______________________________\n",
            "\n",
            "Number of Proliferate images : 79 \n",
            "______________________________\n",
            "\n",
            "Number of severe images : 48 \n",
            "______________________________\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"classes names :\", (data), \"\\n______________________________\\n\")\n",
        "print(\"Number of classes :\", len(data), \"\\n______________________________\\n\")\n",
        "print(\"Number of Healty images :\", len(healthy), \"\\n______________________________\\n\")\n",
        "print(\"Number of Mild images :\", len(mild),  \"\\n______________________________\\n\")\n",
        "print(\"Number of Moderate images :\", len(moderate),  \"\\n______________________________\\n\")\n",
        "print(\"Number of Proliferate images :\", len(proliferate),  \"\\n______________________________\\n\")\n",
        "print(\"Number of severe images :\", len(severe),  \"\\n______________________________\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ieSBZ-u6QCl"
      },
      "outputs": [],
      "source": [
        "main_folder_path = '/content/drive/MyDrive/Dataset2'\n",
        "IMG_SIZE = (128, 128)\n",
        "BATCH_SIZE = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mw4r-CzH6SWU",
        "outputId": "042e77cd-67e3-4795-aed8-936331573f6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 238 images belonging to 5 classes.\n",
            "Found 57 images belonging to 5 classes.\n",
            "Class labels: {'Healthy': 0, 'Mild DR': 1, 'Moderate DR': 2, 'Proliferate DR': 3, 'Severe DR': 4}\n"
          ]
        }
      ],
      "source": [
        "datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    validation_split=0.2,\n",
        "    horizontal_flip=True,\n",
        "    rotation_range=20,\n",
        "    zoom_range=0.2,\n",
        ")\n",
        "\n",
        "# Load data\n",
        "train_data_gen = datagen.flow_from_directory(\n",
        "    main_folder_path,\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "val_data_gen = datagen.flow_from_directory(\n",
        "    main_folder_path,\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    subset='validation'\n",
        ")\n",
        "print(\"Class labels:\", train_data_gen.class_indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CnCxYXNk6VwU",
        "outputId": "3ce5dd35-216d-4fe9-b852-b62b6874ecce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - accuracy: 0.2272 - loss: 1.7727 - val_accuracy: 0.2632 - val_loss: 1.5889\n",
            "Epoch 2/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 1s/step - accuracy: 0.2526 - loss: 1.6003 - val_accuracy: 0.2456 - val_loss: 1.5826\n",
            "Epoch 3/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 901ms/step - accuracy: 0.1865 - loss: 1.5813 - val_accuracy: 0.2632 - val_loss: 1.5498\n",
            "Epoch 4/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 1s/step - accuracy: 0.2697 - loss: 1.5440 - val_accuracy: 0.3509 - val_loss: 1.4399\n",
            "Epoch 5/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 961ms/step - accuracy: 0.3195 - loss: 1.4750 - val_accuracy: 0.3333 - val_loss: 1.4121\n",
            "Epoch 6/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - accuracy: 0.3444 - loss: 1.4184 - val_accuracy: 0.2982 - val_loss: 1.4856\n",
            "Epoch 7/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 1s/step - accuracy: 0.3719 - loss: 1.4366 - val_accuracy: 0.2632 - val_loss: 1.4225\n",
            "Epoch 8/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 896ms/step - accuracy: 0.2950 - loss: 1.4827 - val_accuracy: 0.2982 - val_loss: 1.4165\n",
            "Epoch 9/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 1s/step - accuracy: 0.3948 - loss: 1.3308 - val_accuracy: 0.3333 - val_loss: 1.4311\n",
            "Epoch 10/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1s/step - accuracy: 0.3595 - loss: 1.3432 - val_accuracy: 0.4211 - val_loss: 1.3488\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras import layers, models\n",
        "\n",
        "def create_cnn(input_shape=(128, 128, 3), num_classes=5):\n",
        "    model = models.Sequential([\n",
        "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Train CNN Model\n",
        "cnn_model = create_cnn()\n",
        "cnn_model.fit(train_data_gen, validation_data=val_data_gen, epochs=10)\n",
        "cnn_model.save('cnn_model.h5')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9_rFhW_6YLZ",
        "outputId": "657bf2a2-977f-45ea-c0db-69c5b61ad00b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 535ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 488ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 258ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 259ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 269ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 256ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 247ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['scaler.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "# Extract features for SVM training\n",
        "X_train_features = []\n",
        "y_train_labels = []\n",
        "\n",
        "for images, labels in train_data_gen:\n",
        "    features = cnn_model.predict(images)\n",
        "    X_train_features.extend(features)\n",
        "    y_train_labels.extend(labels)\n",
        "    if len(X_train_features) >= train_data_gen.samples:\n",
        "        break\n",
        "\n",
        "# Train SVM\n",
        "svm_model = svm.SVC(kernel='linear', probability=True)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_features)\n",
        "svm_model.fit(X_train_scaled, np.argmax(y_train_labels, axis=1))\n",
        "\n",
        "# Save SVM and scaler\n",
        "joblib.dump(svm_model, 'svm_model.pkl')\n",
        "joblib.dump(scaler, 'scaler.pkl')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oasoD0rF6u4-"
      },
      "outputs": [],
      "source": [
        "def ensemble_predict(image, cnn_model, svm_model, scaler):\n",
        "    cnn_pred = cnn_model.predict(image)[0]\n",
        "    cnn_class = np.argmax(cnn_pred)\n",
        "\n",
        "    # Flatten and scale image for SVM\n",
        "    image_flatten = image.flatten().reshape(1, -1)\n",
        "    image_scaled = scaler.transform(image_flatten)\n",
        "    svm_pred = svm_model.predict_proba(image_scaled)[0]\n",
        "    svm_class = np.argmax(svm_pred)\n",
        "\n",
        "    # Combine predictions (majority voting)\n",
        "    combined_pred = cnn_pred + svm_pred\n",
        "    final_class = np.argmax(combined_pred)\n",
        "\n",
        "    classes = ['Healthy', 'Mild DR', 'Moderate DR', 'Proliferate DR', 'Severe DR']\n",
        "    return classes[final_class]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1CUwrgr7fLq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f5d0239-6302-49fa-f347-930ece2e1dff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.10/dist-packages (5.9.1)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2.1)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.115.6)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio) (0.5.0)\n",
            "Requirement already satisfied: gradio-client==1.5.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.5.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.25.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.27.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (11.0.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.3)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.8.4)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.41.3)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.13.2)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.15.1)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.34.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.5.2->gradio) (2024.10.0)\n",
            "Requirement already satisfied: websockets<15.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.5.2->gradio) (14.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (3.16.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.27.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (2.2.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install gradio\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 628
        },
        "id": "6nm1u5TX8K2R",
        "outputId": "3f2ad984-1729-4d7e-99b1-16ee45f3ceb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://fda0b029d483159fbc.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://fda0b029d483159fbc.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import gradio as gr\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "import joblib\n",
        "from PIL import Image\n",
        "\n",
        "# Load your pre-trained models and scaler\n",
        "cnn_model = load_model('cnn_model.h5')  # Load CNN model\n",
        "svm_model = joblib.load('svm_model.pkl')  # Load SVM model\n",
        "scaler = joblib.load('scaler.pkl')  # Load scaler for SVM preprocessing\n",
        "\n",
        "# Define classes for diabetic retinopathy stages\n",
        "classes = ['Healthy', 'Mild DR', 'Moderate DR', 'Proliferate DR', 'Severe DR']\n",
        "\n",
        "# Define the ensemble prediction function\n",
        "def ensemble_predict(img):\n",
        "    # Preprocess image for CNN\n",
        "    img_resized = img.resize((128, 128))\n",
        "    img_array = np.array(img_resized) / 255.0\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "    # Predict using CNN\n",
        "    cnn_pred = cnn_model.predict(img_array)[0]\n",
        "\n",
        "    # Preprocess image for SVM\n",
        "    img_flatten = img_array.flatten().reshape(1, -1)\n",
        "    img_scaled = scaler.transform(img_flatten)\n",
        "\n",
        "    # Predict using SVM\n",
        "    svm_pred = svm_model.predict_proba(img_scaled)[0]\n",
        "\n",
        "    # Combine predictions using a weighted average (example weights: 0.6 CNN, 0.4 SVM)\n",
        "    combined_pred = 0.6 * cnn_pred + 0.4 * svm_pred\n",
        "    final_class = np.argmax(combined_pred)\n",
        "\n",
        "    # Return the final class\n",
        "    return f\"Predicted Stage: {classes[final_class]}\"\n",
        "\n",
        "# Create Gradio interface\n",
        "iface = gr.Interface(\n",
        "    fn=ensemble_predict,  # Prediction function\n",
        "    inputs=gr.Image(type=\"pil\"),  # Image input\n",
        "    outputs=\"text\",  # Text output (class label)\n",
        "    title=\"Diabetic Retinopathy Detection App\",\n",
        "    description=\"Welcome to our Diabetic Retinopathy Detection App! This app utilizes deep learning models to detect diabetic retinopathy in retinal images. Diabetic retinopathy is a common complication of diabetes and early detection is crucial for effective treatment.\",\n",
        "    examples=[\n",
        "        \"/content/drive/MyDrive/Dataset2/Healthy/Healthy_15.png\",\n",
        "        \"/content/drive/MyDrive/Dataset2/Mild DR/Mild_DR_221.png\",\n",
        "        \"/content/drive/MyDrive/Dataset2/Moderate DR/Moderate_DR_358.png\",\n",
        "        \"/content/drive/MyDrive/Dataset2/Severe DR/Severe DR_10.png\",\n",
        "    ],\n",
        ")\n",
        "\n",
        "# Launch the Gradio interface\n",
        "if __name__ == \"__main__\":\n",
        "    iface.launch(share=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://bhimrazy-diabetic-retinopathy-detection.hf.space"
      ],
      "metadata": {
        "id": "wiFukD-8U4ut"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h7U_-1kd8LdP"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}